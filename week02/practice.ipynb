{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rational-latitude",
   "metadata": {},
   "source": [
    "# Practice 2. PyTorch Distributed and data-parallel training\n",
    "In this assignment, we'll be going over the distributed part of the PyTorch library. \n",
    "\n",
    "\n",
    "This notebook is inspired by an awesome [PyTorch tutorial](https://pytorch.org/tutorials/intermediate/dist_tuto.html). If you wish to dive deeper into this topic, feel free to read this tutorial, as well as the [docs](https://pytorch.org/docs/stable/distributed.html) themselves.\n",
    "\n",
    "For now, let's import all required libraries and define a function which will create the process group:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "divided-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "import random\n",
    "\n",
    "def init_process(rank, size, fn, master_port, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = str(master_port)\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "    \n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-chair",
   "metadata": {},
   "source": [
    "First, we'll run a very simple function with torch.distributed.barrier. The cell below prints in the first process and then prints in all other processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hungry-chest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started 0\n",
      "Started 1Started 2\n",
      "\n",
      "Started 3\n"
     ]
    }
   ],
   "source": [
    "def run(rank, size):\n",
    "    \"\"\" Distributed function to be implemented later. \"\"\"\n",
    "    if rank!=0:\n",
    "        dist.barrier()\n",
    "    print(f'Started {rank}',flush=True)\n",
    "    if rank==0:\n",
    "        dist.barrier()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    size = 4\n",
    "    processes = []\n",
    "    port = random.randint(25000, 30000)\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, args=(rank, size, run, port))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-intermediate",
   "metadata": {},
   "source": [
    "Let's implement a classical ping-pong application with this paradigm. We have two processes, and the goal is to have P1 output 'ping' and P2 output 'pong' without any race conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "closed-nashville",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ping\n",
      "pong\n",
      "ping\n",
      "pong\n",
      "ping\n",
      "pong\n",
      "ping\n",
      "pong\n",
      "ping\n",
      "pong\n",
      "ping\n",
      "pong\n",
      "ping\n",
      "pong\n",
      "ping\n",
      "pong\n",
      "ping\n",
      "pong\n",
      "ping\n",
      "pong\n"
     ]
    }
   ],
   "source": [
    "def run_pingpong(rank, size):\n",
    "    \"\"\" Distributed function to be implemented later. \"\"\"\n",
    "    num_iter = 10\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        if rank==0:\n",
    "            dist.barrier()\n",
    "            print('ping', flush=True)\n",
    "            dist.barrier()\n",
    "        if rank==1:\n",
    "            dist.barrier()\n",
    "            dist.barrier()\n",
    "            print('pong', flush=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    port = random.randint(25000, 30000)\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, args=(rank, size, run_pingpong, port))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-foster",
   "metadata": {},
   "source": [
    "## Task 1 (0.1 score)\n",
    "Generalize the above function to sequential printing for N processes without race conditions in the following order of messages:\n",
    "```\n",
    "Process 0\n",
    "Process 1\n",
    "Process 2\n",
    "Process 3\n",
    "...\n",
    "Process N-1\n",
    "```\n",
    "\n",
    "\n",
    "# Point-to-point communication\n",
    "The functions below show that it's possible to send data from one process to another with `torch.distributed.send/torch.distributed.recv`. Also, these functions have an immediate (asynchronous) version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "varying-nightmare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank Rank   01   has data  has data   tensor(1.)tensor(0.)\n",
      "\n",
      "Rank  0 Rank  has data  tensor(1.)\n",
      " 1  has data  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def run_sendrecv(rank, size):\n",
    "    tensor = torch.zeros(1)+int(rank==0)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "    if rank == 0:\n",
    "        # Send the tensor to process 1\n",
    "        dist.send(tensor=tensor, dst=1)\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    port = random.randint(25000, 30000)\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, args=(rank, size, run_sendrecv, port))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "statistical-serve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 started sendingRank 1 started receiving\n",
      "\n",
      "Rank Rank   10   has data   has data  tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Non-blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def run_isendrecv(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    req = None\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        req = dist.isend(tensor=tensor, dst=1)\n",
    "        print('Rank 0 started sending')\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        req = dist.irecv(tensor=tensor, src=0)\n",
    "        print('Rank 1 started receiving')\n",
    "    req.wait()\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    port = random.randint(25000, 30000)\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, args=(rank, size, run_isendrecv, port))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-sacramento",
   "metadata": {},
   "source": [
    "# Collective communication and All-Reduce\n",
    "Now, let's run a simple All-Reduce example which computes the sum across all workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sustained-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 has data 45Rank 5 has data 45Rank 6 has data 45Rank 2 has data 45Rank 7 has data 45Rank 3 has data 45Rank 4 has data 45Rank 9 has data 45\n",
      "Rank 8 has data 45\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rank 1 has data 45\n"
     ]
    }
   ],
   "source": [
    "\"\"\" All-Reduce example.\"\"\"\n",
    "\n",
    "def run_allreduce(rank, size):\n",
    "    tensor = torch.full((1,),rank)\n",
    "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
    "    print(f'Rank {rank} has data {tensor[0]}')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    size = 10\n",
    "    processes = []\n",
    "    port = random.randint(25000, 30000)\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, args=(rank, size, run_allreduce, port))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unsigned-conservative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_allreduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_allreduce.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "\"\"\" All-Reduce example.\"\"\"\n",
    "\n",
    "from functools import partial\n",
    "\"\"\" All-Reduce example.\"\"\"\n",
    "\n",
    "def run_allreduce(rank, size):\n",
    "    tensor = torch.ones(1)\n",
    "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "    \n",
    "def init_process(rank, size, fn, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    size = 10\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, args=(rank, size, run_allreduce))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "moderate-latex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  0  has data  tensor(10.)\r\n",
      "Rank  9  has data  tensor(10.)\r\n",
      "Rank  8  has data  tensor(10.)\r\n",
      "Rank  7  has data  tensor(10.)\r\n",
      "Rank  6  has data  tensor(10.)\r\n",
      "Rank  5  has data  tensor(10.)\r\n",
      "Rank  3  has data  tensor(10.)\r\n",
      "Rank  4  has data  tensor(10.)\r\n",
      "Rank  1  has data  tensor(10.)\r\n",
      "Rank  2  has data  tensor(10.)\r\n"
     ]
    }
   ],
   "source": [
    "! ./run_allreduce.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "chronic-tokyo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_allreduce_spawn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_allreduce_spawn.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "\"\"\" All-Reduce example.\"\"\"\n",
    "\n",
    "def run_allreduce(rank, size):\n",
    "    tensor = torch.ones(1)\n",
    "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "    \n",
    "def init_process(rank, size, fn, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    size = 10\n",
    "\n",
    "    fn = partial(init_process, size=size, fn=run_allreduce, backend='gloo')\n",
    "    torch.multiprocessing.spawn(fn, nprocs=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extreme-trigger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  8  has data  tensor(10.)\r\n",
      "Rank  6  has data  tensor(10.)\r\n",
      "Rank  4  has data  tensor(10.)\r\n",
      "Rank  0  has data  tensor(10.)\r\n",
      "Rank  9  has data  tensor(10.)\r\n",
      "Rank  7  has data  tensor(10.)\r\n",
      "Rank  3  has data  tensor(10.)\r\n",
      "Rank  5  has data  tensor(10.)\r\n",
      "Rank  2  has data  tensor(10.)\r\n",
      "Rank  1  has data  tensor(10.)\r\n"
     ]
    }
   ],
   "source": [
    "! ./run_allreduce_spawn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "federal-structure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom_allreduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_allreduce.py\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "import random\n",
    "\n",
    "def init_process(rank, size, fn, master_port, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = str(master_port)\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "\n",
    "def allreduce(send):\n",
    "    rank = dist.get_rank()\n",
    "    size = dist.get_world_size()\n",
    "    send_buff = send.clone()\n",
    "    recv_buff = send.clone()\n",
    "    accum = send.clone()\n",
    "\n",
    "    left = ((rank - 1) + size) % size\n",
    "    right = (rank + 1) % size\n",
    "\n",
    "    for i in range(size - 1):\n",
    "        if i % 2 == 0:\n",
    "            # Send send_buff\n",
    "            send_req = dist.isend(send_buff, right)\n",
    "            dist.recv(recv_buff, left)\n",
    "            accum[:] += recv_buff[:]\n",
    "        else:\n",
    "            # Send recv_buff\n",
    "            send_req = dist.isend(recv_buff, right)\n",
    "            dist.recv(send_buff, left)\n",
    "            accum[:] += send_buff[:]\n",
    "        send_req.wait()\n",
    "    send[:] = accum[:]\n",
    "\n",
    "def run_allreduce(rank, size):\n",
    "    \"\"\" Simple point-to-point communication. \"\"\"\n",
    "    tensor = torch.full((1,), rank, dtype=torch.float)\n",
    "    allreduce(tensor)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    size = 50\n",
    "    processes = []\n",
    "    port = random.randint(25000, 30000)\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, args=(rank, size, run_allreduce, port))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pressing-update",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1225"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "covered-pickup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  22  has data  tensor(1225.)\n",
      "Rank  27  has data  tensor(1225.)\n",
      "Rank  28  has data  tensor(1225.)\n",
      "Rank  29  has data  tensor(1225.)\n",
      "Rank  40  has data  tensor(1225.)\n",
      "Rank  20  has data  tensor(1225.)\n",
      "Rank  38  has data  tensor(1225.)\n",
      "Rank  45  has data  tensor(1225.)\n",
      "Rank  23  has data  tensor(1225.)\n",
      "Rank  42  has data  tensor(1225.)\n",
      "Rank  32  has data  tensor(1225.)\n",
      "Rank  24  has data  tensor(1225.)\n",
      "Rank  46  has data  tensor(1225.)\n",
      "Rank  21  has data  tensor(1225.)\n",
      "Rank  30  has data  tensor(1225.)\n",
      "Rank  7  has data  tensor(1225.)\n",
      "Rank  9  has data  tensor(1225.)\n",
      "Rank  19  has data  tensor(1225.)\n",
      "Rank  49  has data  tensor(1225.)\n",
      "Rank  16  has data  tensor(1225.)\n",
      "Rank  39  has data  tensor(1225.)\n",
      "Rank  15  has data  tensor(1225.)\n",
      "Rank  47  has data  tensor(1225.)\n",
      "Rank  18  has data  tensor(1225.)\n",
      "Rank  31  has data  tensor(1225.)\n",
      "Rank  2  has data  tensor(1225.)\n",
      "Rank  5  has data  tensor(1225.)\n",
      "Rank  8  has data  tensor(1225.)\n",
      "Rank  0  has data  tensor(1225.)\n",
      "Rank  14  has data  tensor(1225.)\n",
      "Rank  4  has data  tensor(1225.)\n",
      "Rank  1  has data  tensor(1225.)\n",
      "Rank  11  has data  tensor(1225.)\n",
      "Rank  10  has data  tensor(1225.)\n",
      "Rank  12  has data  tensor(1225.)\n",
      "Rank  41  has data  tensor(1225.)\n",
      "Rank  44  has data  tensor(1225.)\n",
      "Rank  33  has data  tensor(1225.)\n",
      "Rank  37  has data  tensor(1225.)\n",
      "Rank  35  has data  tensor(1225.)\n",
      "Rank  48  has data  tensor(1225.)\n",
      "Rank  43  has data  tensor(1225.)\n",
      "Rank  36  has data  tensor(1225.)\n",
      "Rank  3  has data  tensor(1225.)\n",
      "Rank  26  has data  tensor(1225.)\n",
      "Rank  6  has data  tensor(1225.)\n",
      "Rank  17  has data  tensor(1225.)\n",
      "Rank  25  has data  tensor(1225.)\n",
      "Rank  34  has data  tensor(1225.)\n",
      "Rank  13  has data  tensor(1225.)\n"
     ]
    }
   ],
   "source": [
    "!python custom_allreduce.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-lesson",
   "metadata": {},
   "source": [
    "# Distributed training\n",
    "\n",
    "Armed with this simple implementation of AllReduce, we can run multi-process distributed training. For now, let's use the model and the dataset from the official MNIST [example](https://github.com/pytorch/examples/blob/master/mnist/main.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "instructional-dictionary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_example.py\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "import random\n",
    "\n",
    "def init_process(args, fn, backend='nccl'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    rank = args.local_rank\n",
    "    dist.init_process_group(backend, rank=rank)\n",
    "    size = dist.get_world_size()\n",
    "    fn(rank, size)\n",
    "    \n",
    "torch.set_num_threads(1)\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "from functools import partial\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(4608, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        output = self.fc2(x)\n",
    "        return output\n",
    "\n",
    "def run_training(rank, size):\n",
    "    torch.manual_seed(1234)\n",
    "    dataset = MNIST('./mnist',transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "    loader = DataLoader(dataset, sampler=DistributedSampler(dataset, size, rank), batch_size=16)\n",
    "    model = Net()\n",
    "    device = torch.device('cuda',rank)\n",
    "    model.to(device)\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank], output_device=[rank])\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                          lr=0.01, momentum=0.5)\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    steps = 0\n",
    "    epoch_loss = 0\n",
    "    for data, target in loader:\n",
    "        data=data.to(device)\n",
    "        target=target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        steps+=1\n",
    "        if steps % 100 == 0:\n",
    "            print(f'Rank {dist.get_rank()}, loss: {epoch_loss / num_batches}')\n",
    "            epoch_loss = 0\n",
    "        \n",
    "def parse_args():\n",
    "    parser=ArgumentParser()\n",
    "    parser.add_argument('--local_rank', type=int)\n",
    "    args=parser.parse_args()\n",
    "    return args\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    args=parse_args()\n",
    "    print(args.local_rank)\n",
    "    init_process(args,fn=run_training, backend='nccl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "compound-trustee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "1\n",
      "0\n",
      "Rank 1, loss: 0.0711741222222646\n",
      "Rank 0, loss: 0.07069784507751464\n",
      "Rank 0, loss: 0.03514039077758789\n",
      "Rank 1, loss: 0.034355521066983544\n",
      "Rank 1, loss: 0.027626613974571227\n",
      "Rank 0, loss: 0.028529569840431212\n",
      "Rank 1, loss: 0.02547050395409266\n",
      "Rank 0, loss: 0.024831330653031666\n",
      "Rank 0, loss: 0.02246965136130651\n",
      "Rank 1, loss: 0.023988804654280344\n",
      "Rank 0, loss: 0.020719586579004922\n",
      "Rank 1, loss: 0.020591482353210448\n",
      "Rank 0, loss: 0.019405658107995986\n",
      "Rank 1, loss: 0.01995779107809067\n",
      "Rank 0, loss: 0.019439678021272024\n",
      "Rank 1, loss: 0.01599694345196088\n",
      "Rank 1, loss: 0.017450609695911407\n",
      "Rank 0, loss: 0.016963590412338574\n",
      "Rank 0, loss: 0.014799549550811449\n",
      "Rank 1, loss: 0.014166126350561777\n",
      "Rank 1, loss: 0.012973578509688378\n",
      "Rank 0, loss: 0.014403845605254173\n",
      "Rank 1, loss: 0.01364186906615893\n",
      "Rank 0, loss: 0.012408630867799123\n",
      "Rank 1, loss: 0.01225809634650747\n",
      "Rank 0, loss: 0.011144915817181269\n",
      "Rank 0, loss: 0.012421928869684537\n",
      "Rank 1, loss: 0.013346474504470825\n",
      "Rank 0, loss: 0.011802775514125823\n",
      "Rank 1, loss: 0.010456619673967361\n",
      "Rank 0, loss: 0.009187962346772353\n",
      "Rank 1, loss: 0.010249189861118793\n",
      "Rank 1, loss: 0.010523318041364351\n",
      "Rank 0, loss: 0.009798831804096698\n",
      "Rank 0, loss: 0.009258631070454916\n",
      "Rank 1, loss: 0.009735991016278664\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node 2 ddp_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-creature",
   "metadata": {},
   "source": [
    "## Task 2 (0.6 score)\n",
    "\n",
    "The above pipeline shows only the basic building blocks of distributed training. Now, let's train something actually interesting! For example, let's take the [CIFAR-100](https://pytorch.org/vision/0.8/datasets.html#torchvision.datasets.CIFAR100) dataset and train a model with **synchronized** Batch Normalization: that is, we average the statistics across workers during each forward pass. Also, implement a version of distributed training which is aware of gradient accumulation: for each batch that doesn't run `optimizer.step`, you can avoid the All-Reduce step altogether. \n",
    "\n",
    "(If the resources allow you) Compare the performance (in terms of both speed and quality) of your distributed training pipeline with [the](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel) [primitives](https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html?highlight=syncbatchnorm#torch.nn.SyncBatchNorm) provided by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-spelling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "severe-aluminum",
   "metadata": {},
   "source": [
    "## Task 3 (0.3 score)\n",
    "\n",
    "For now, we only aggregate the gradients across different workers during training. But what if we want to run distributed validation on a large dataset as well? In this assignment, you have to implement distributed metric aggregation: shard the dataset across different workers (with `scatter`), compute accuracy for each subset on its respective worker and then average the metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-columbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
